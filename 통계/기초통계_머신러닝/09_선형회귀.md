# 통계및머신러닝
## 10) 선형회귀
### 회귀
- 오차제곱의 합이 최소화가 되는 회귀 계수들은 경사하강법을 사용해 찾을 수 있다
- 경사하강법이란 오차제곱의 합을 n 으로 나눈 SSE/n = MSE 를 =0 으로 미분하여 구하는 방법이다
- 경사하강법에서는 기울기가 감소하는 방향으로 이동하는데 일정한 보폭 혹은 점차 감소하는 보폭으로 이동과 기울기 계산을 반복한다
- 보폭을 학습률 이라 하는데 크면 발산의 위험이 있고 작으면 지역 최저점에서 벗어나지 못할 수 있다
- 다중공선성은 VIF(Variation Index Factor) 가 10이상인 설명변수를 말한다
  - 해결방법은 해당 독립변수를 제거할 수 있으나 무조건은 아니다
- 회귀모형에서 명목형 변수는 One hot encoding 혹은 가변수화 방법 등으로 처리 할 수 있다

### Penalized Linear Regression
- 변수가 많은 경우 혹은 고차원에서 최적의 변수 집합을 구하기 위한 전통적인 변수 선택 기법들이다
  - Ridge Regression : 과적합을 피하고 일반화 성능을 강화하기 위해 가중치를 줄여주는 방법이다
  - LASSO (Least Absolute Shrinkage Selector Operator) : 해당 변수를 모델에서 제거하는 효과를 갖는다
  - Elastic Net : 위 두가지를 합친 개념이다

### Logistic Regression
- 지도학습의 일종으로 수치가 나오긴 하지만 해당 수치를 통해 Feature 값들과 label 값의 class 간 관계를 학습하여 관측된 데이터의 class 를 예측(분류) 하는 방법이다
- 회귀 분서깅지만 분류하는 방법으로 이해하면 된다
- 종속변수가 범주형인 경우 Classification 이다
- 이진 분류 : Label 값으로 0/1, Y/N 등 과 같이 두 가지 class 만 가능한 것을 말한다
- 독립변수들의 선형 결합과 종속변수의 class 간의 확률적 관계를 학습한다
- 승산비, 로짓(로그)함수, 역함수의 변환 과정을 거쳐서 logistic(x) = e(x승) / (1 + e(x승)) 함수로 만들어진다
- 비용함수의 경우 우도함수(Likelihood) 는 최대화하고 Loss(Cross-entrophy) 의 경우 최소가 되도록 구성한다

### 임계값
- 문턱의 개념으로 기본 임계값은 0.5이다
- 임계값을 낮추면 민감도(Recall) 이 높아져 오분류가 높아지더라도 Y=1 인 경우를 최대한 분류한다
- 임계값을 높이면 Precision 이 높아져 알파 오류를 최소화 하는 경향이 있다
- 선형회귀와 달리 로지스틱 회귀는 종속변수가 logit 확률로부터 도출한 class 값(A or B) 이다
- 회귀계수는 독립변수 값이 1 단위 증가할 때 log(odds) 변화량이다

### KNN
- KNN 은 분류와 회귀 문제를 모두 다룰 수 있으며 비모수 방식이며 instance-based 알고리즘이다
- Train 과 Test 세트로 데이터를 분리하지만 실제로 Train 은 존재하지 않는 게으른 알고리즘이다
- 지도학습 기법으로 최적의 K 에 따라 성능이 달라지며 이는 실험을 통해 찾는 수 밖에 없다
- 실험데이터가 작을 때 k 값은 대개 홀수로 정하며, 데이터가 많아질수록 k 는 커진다
- K = 1 이면 무조건 과대적합이며 커질수록 과소적합이 된다
- 거리 측정은 유클리디안 거리와 맨해턴 거리를 통해 측정하며 Feature 의 경우 모두 수치형 데이터가 요구된다 (범주형은 가변수화)
- 거리 개념을 사용하는 알고리즘은 Normalization 을 항상 검토할 필요가 있다
- 단순한 알고리즘으로 데이터에 대한 기본 가정이 없으므로 비선형 데이터에 유용하며 대체로 우수한 결과를 보여준다
- 상대적으로 높은 계싼 비용이 들고 데이터 양이 매우 큰 경우 계산 속도가 느려지고 데이터의 지역 구조에 민감하며 최적 k 값을 구하기 어렵다

### Naive Bayes 분류
- 베이즈 법칙에 기반한 분류기법으로 Feature 들이 확률적으로 독립이라는 가정으로 단순화가 상당히 효과적으로 문제를 풀어갈 수 있게 한다
- 베이즈 분류기는 Feature 들이 모두 동등하게 중요하며 모두 독립적이라는 가정이다
- 베이즈 정리란 사전확률과 조건부확률의 곱이 최대가 되는 값을 찾자는 것이다(의사결정의 개념)
- 샘플의 개수가 작으면 분자, 분모에 특정상수를 더하는 Laplace Smoothing 을 사용한다
- 과소적합에서 발생하는 double 범위 문제를 해결하기 위해 exp(지수) 와 로그를 사용한다
- 가장 단순한 지도학습 방법 가운데 하나로 빠르고 적은 양의 데이터로도 정확하며 비용이 적고 대량 데이터 세트에서도 이산형 데이터라면 높은 성능을 보인다
- 현실적으로 독립이라는 조건이 힘들며 연속형 수치 데이터가 많은 경우 이상적이지 않고 조건부 확률이 0이 되는 문제가 존재하며 데이터 사이즈가 작으면 과대적합 가능성이 있다

### 의사결정 나무
- 의사결정 나무란 의사결정 규칙을 나무구조로 도식화하여 관심집단을 분류하거나 예측하는 계량적 분석방법이다
- Leaf Node 에 Target Value 가 들어가는 것이 분류이며 평균값이 들어가는게 회귀이다
- 현재 Target value 가 모두 같은 값이면 알고리즘을 멈추며 그렇지 않으면 최적의 요인과 기준값으로 Split 하는 것을 반복한다
- 수치형, 범주형 데이터 모두 적용 가능하기 때문에 자료 가공이 거의 필요없으나 휴리스틱에 근거한 알고리즘으로 전역 최적화를 얻지 못할 수 있으며 과적합이 쉽게 발생한다
- 불순도를 최소화하는 방향으로 분리기준을 정하나 자료를 직교하는 직선으로 밖에 분할하지 못한다
- 범주형 목표변수에서 불순도 측정 기법은 Entropy Reduction, Gini Reduction, Chi-square test 가 있다
- 연속형 목표변수에서 불순도 측정 기법은 F-test, Reduction of Variance 가 있다
- 모든 마디가 순수한 상태가 되면 정지하며 트리의 깊이가 지정한 값에 도달하거나 노드의 크기가 지정한 값보다 작거나 불순도의 감소량이 지정된 값보다 작으면 정지한다
- 가지치기란 상향식으로 가지를 쳐서 sub-tree 를 마디로 대체하는 것이며 끝마디의 클래스는 sub-tree 내의 클래스의 다수결로 정하며 가지치기의 결과로 generalization error 가 개선되어야 한다
- 앙상블 기법에는 Bagging, Boostring 이 있다
  - Bagging 은 병렬처리 기법으로 Bootstrap Aggregation 즉 복원임의추출로 생긴 그룹의 개수만큼 의사결정 나무를 만드는 것이다
  - Boosting 은 복원 랜덤 샘플링을 하고 잔차를 줄이는 방향으로 단순한 모델(약한 분류기)을 만들어서 의사결정 나무를 활용하는 것이다

### Clustering
- 클러스터링은 패턴을 찾아 범주화하고 새로운 데이터를 실시간으로 처리하려는 비지도 학습을 사용한다
  - 비지도 학습 : Clustering, Association Rule, Dimension Reduction
- 클러스터링은 분석가의 해석이 중요하며 상호 배반적 군집이란 여러 군집 가운데 하나에만 속하는 경우를 말한다
- 표준화 방법으로는 Min-Max scaling, Z-Score, Robust scaling, Max-Absolute scaling 이 있다
- 계층적 군집 분석은 군집을 병합하며 하나의 군집이 남을 때 까지 진행하는 것으로 병합방법에는 최단 연결법, 최장 연결법, 평균 연결법, 중심 연결법 등이 있다
- Ward 의 방법이란 최소 분산 연결법으로 SSE 의 증가분이 최소가 되도록 군집을 병합시키는 것을 말한다
- 덴드로그램은 군집화 과정을 간략하게 나타내는 나무 형태의 도표를 말한다
- 계층적 군집 분석은 사전에 군집의 수를 결정할 필요가 없지만 여러 군집에 할당이 불가능하고 자료수가 늘어나면 계산량이 크게 증가하며 분석이 쉽지 않다
- 비계층적 군집 분석은 사전에 정한 K 개의 군집으로 나누어 분석하는 방법이다
- K-평균 군집화 알고리즘은 군집의 중심을 구한 뒤 해당 중심을 기준으로 다시 군집을 할당하는 과정을 반복한다
  - 군집의 변화가 없거나 중심점의 이동이 임계값(왜곡값) 이하일 때 멈춘다
  - k 를 정하는 방법으로는 Elbow method, Silhouette coefficient 가 있다
  - 엘보 메소드는 왜곡이 작으면서 크지 않은 k 를 최적으로 하는 것이며 실루엣 계수는 숫자가 클수록 좋다
- 연관 규칙은 선행과 후행 형식으로 항목들 사이의 모든 가능한 규칙들을 조사한다
  - 선행, 후행은 각각 중복이 없는 집합이며 선행과 후행 사이에 공통 원소가 없어야 한다
  - 너무 많은 Rule 이 도출되는 빈발지표이므로 이를 적절히 제한할 기준이 필요하다
  - 널리 사용되는 지표는 support, confidence, lift 가 있다
  - 지지도는 효율성, 신뢰도는 효율성과 유효성, 향상도는 유효성 지표이다
  - 지지도는 전체 중의 해당 집합의 개수이며 신뢰도는 지지도(x->y)/지지도(x) 이고 향상도는 신뢰도(x->y)/지지도(y) 이다
- FP Tree 성장 알고리즘은 연관성 규칙에서 연관 규칙으로 쓰이는 알고리즘이다
- 거래패턴을 간단 명료하게 파악가능하지만 연관성만을 의미하며 (인과관계아님) 너무 많은 Rule 이 생성되고 새로운 항목이 추가되는 경우 추천이 힘든 Cold Start 단점이 있다

### 차원축소
- 차원이 늘어나면 필요한 데이터가 늘어나며 과대 적합의 가능성이 증가된다
- 과대적합이 발생함에 따라 다중공선성이 발생하고 편중 혹은 희박한 데이터로 인한 문제들이 발생한다
- 차원축소는 학습 데이터 내의 변수의 개수를 줄여서 저차원 데이터로 변환하는 것이며 시각화 가능하고 다중공선성을 제거 가능하나 정보의 일부가 손실되고 원데이터 관점의 해석이 어렵다
- Filter Methods 는 데이터 세트를 의미 있는 feature 만 담은 subset 으로 필터하여 축소하는 것으로 다른 방법보다 단순하고 빠르며 경제적이며 서로의 의존성은 무시한다
- Wrapper Methods 는 머신러닝 모형을 사용하여 성능 평가를 하여 의미 있는 feature 만 담은 subset 으로 축소하는 방법이다
- 모델 간 비교에는 R 스퀘어와 향상된 R 스퀘어를 지표로 사용한다